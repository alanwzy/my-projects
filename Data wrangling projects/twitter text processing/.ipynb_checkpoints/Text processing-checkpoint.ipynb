{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing\n",
    "#### Creator Name: ZHIYIN WANG\n",
    "\n",
    "Date: 13/09/2020\n",
    "\n",
    "Version: 2.0\n",
    "\n",
    "Environment: Python 3.6.0\n",
    "\n",
    "Libraries used:\n",
    "* pandas 0.19.2 (for data frame, included in Anaconda Python 3.6) \n",
    "* nltk 3.2.2 (Natural Language Toolkit, included in Anaconda Python 3.6)\n",
    "* nltk.collocations (for finding bigrams, included in Anaconda Python 3.6)\n",
    "* nltk.tokenize (for tokenization, included in Anaconda Python 3.6)\n",
    "* nltk.stem (for stemming words, included in Anaconda Python 3.6)\n",
    "* nltk.util (for finding bigrams, included in Anaconda Python 3.6)\n",
    "* nltk.probability (for counting token frequency, included in Anaconda Python 3.6)\n",
    "* sklearn.feature_extraction.text (for creating countvector matrix, included in Anaconda Python 3.6)\n",
    "* langid (for classifying language of text, not included in Anaconda Python 3.6)\n",
    "* itertools (for connecting all values in library together, included in Anaconda Python 3.6)\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment comprises the execution of different text processing and analysis tasks applied to documents in Excel format. There are a total of 81 excels in one 17.7 MB file named `31436285.xlsx`. The required tasks are the following:\n",
    "\n",
    "1. Generate the corpus vocabulary with the same structure as sample_vocab.txt . Please note that the vocabulary must be sorted alphabetically.\n",
    "\n",
    "2. For each day (i.e., sheet in your excel file), calculate the top 100 frequent unigram and top-100 frequent bigrams according to the structure of the sample_100uni.txt and sample_100bi.txt . If you have less than 100 bigrams for a particular day, just include the top-n bigrams for that day (n<100).\n",
    "\n",
    "3. Generate the sparse representation (i.e., doc-term matrix) of the excel file according to the structure of the sample_countVec.txt\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries \n",
    "\n",
    "install and import from libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in c:\\users\\alan wang\\anaconda3\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\alan wang\\anaconda3\\lib\\site-packages (from langid) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langid import classify\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk import MWETokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examing and Loading data\n",
    "\n",
    "first, lets read the excel file into excel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the excel data \n",
    "excel_data = pd.ExcelFile('31436285.xlsx')\n",
    "#excel_data = pd.ExcelFile(\"sample.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The excel data contains many excel sheets\n",
    "\n",
    "Create a dictionary named \"data\" to contain all the excel sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a library to contain data\n",
    "data = {}        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading every sheet in the excel file into our data dictionary, the excel sheets need to be reformated first.\n",
    "\n",
    "The sheets contains NAN rows and the column need to be renamed.\n",
    "\n",
    "dropna(0, how = \"all\") is applied here to drop all rows with only NA. In excel sheets with incorrect column names, the column names should be renamed by the first row of that sheet with rename()\n",
    "\n",
    "After the adjustment to rows and column names, remember to reindex the sheet so that erros don't occur in later loops.\n",
    "\n",
    "Next, the tweets in each sheet need to be in english, need to drop the tweets that are not in english. Non-english tweets are identified by classify() function. Tweets with classify() results not \"en\" need to be droped. However, drop the row while looping also changes index, thus cause issues while looping. All the index of tweets that are not in english will be appended in to the drop_list and drop together at once later using drop().\n",
    "\n",
    "Now, we have all the english tweets in the excel sheets in correct format. Add them to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read every sheet in the excel and drop the empty columns in every sheet\n",
    "for i in range(len(excel_data.sheet_names)):\n",
    "    # read excel sheet\n",
    "    df = excel_data.parse(i)\n",
    "\n",
    "    # drop na rows\n",
    "    df = df.dropna(0, how = \"all\")\n",
    "    \n",
    "    # if the column names are not correct\n",
    "    # rename the column names with the first row and drop the first row \n",
    "    if 'text' not in df.columns:\n",
    "        df.rename(columns = df.iloc[0], inplace = True)\n",
    "        df = df[1:]\n",
    "        \n",
    "    # Reindex rows\n",
    "    df.index = range(len(df.index))\n",
    "    \n",
    "    # create a list to contain the index of rows to drop\n",
    "    drop_list = []\n",
    "    \n",
    "    # check if the text is in english, only keep english twitters\n",
    "    # append index of rows need to be droped to the list\n",
    "    for j in range(len(df.text)):\n",
    "        if classify(str(df.text[j]))[0] != \"en\":\n",
    "            # append index to drop_list\n",
    "            drop_list.append(j)\n",
    "    \n",
    "    # drop the rows where twitters are not in english\n",
    "    df.drop(drop_list,inplace = True) \n",
    "    \n",
    "    # append the excel sheet to library\n",
    "    data[excel_data.sheet_names[i]] = \"\\n\".join(map(str,df.text.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transform data to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate vocab, unigram, bigram, tokens need to be generated from the raw text data first.\n",
    "\n",
    "create new dictionary to hold tokens in each sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the token dictionary \n",
    "\n",
    "# initial token dictionary \n",
    "token_dic = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to help tokenize the entire excel dictionary.\n",
    "\n",
    "RegexpTokenizer is used here with given the regex (\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\").\n",
    "The regular expression catches any word and words that contain \"-\". Applying it to tweets in excel sheets will capture every single word in tweets as a token.\n",
    "\n",
    "Before tokenize the tweets, let's make all text in tweets to lower case using lower() for consistency.\n",
    "\n",
    ".tokenize() is used here to tokenize the tweets and the resulted tokens will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for creating unigram tokens\n",
    "def tokenizeRawData(text):\n",
    "    # regex to extract token\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "    \n",
    "    # transform into lower case to keep all words consistent \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # create unigram tokens\n",
    "    all_uni_tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    return(all_uni_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through every excel sheet and apply the tokenize function to every tweets. The result tokens will be stored in the new token dictionary created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create token dictionary with keys = dates, value = tokens, all tokens in lower case\n",
    "for k,v in data.items():\n",
    "    tokens = tokenizeRawData(v)\n",
    "    token_dic[k] = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output has following requirements:\n",
    "\n",
    "1. The context-independent and context-dependent (with the threshold set to more than 60 days ) stop words must be removed from the vocab. The provided context-independent stop words list (i.e, stopwords_en.txt ) must be used.\n",
    "2. Tokens should be stemmed using the Porter stemmer.\n",
    "3. Rare tokens (with the threshold set to less than 5 days ) must be removed from the vocab.\n",
    "4. Creating sparse matrix using countvectorizer.\n",
    "5. Tokens with the length less than 3 should be removed from the vocab.\n",
    "6. First 200 meaningful bigrams (i.e., collocations) must be included in the vocab using PMI measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As announced, context dependent stopwords need to be removed first before stemming.\n",
    "\n",
    "To find tokens with document frequency < 5 and >60, FreqDist() can be used to count the frequency of tokens.\n",
    "\n",
    "First, chain.from_iterable is used to put all tokens in the token dictionary together, set() is applied here to only keeps unique token in every day. From there we got all unique tokens gather together in a list. FreqDist() the list and store name it words_freq.\n",
    "\n",
    "Next step is to find the tokens in words_freq with frequency < 5 and > 60. Words_freq is like a dictionary with key = token_name and value = frequency count. \n",
    "Thus, find the context dependent stopwords with loop and name the set as a variable. This is a set here as the results from the loop has duplicates that we don't need and set() is faster for future iteratoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Unigrams \n",
    "\n",
    "# create vocab set contain unique words based on the word list\n",
    "# And count frequency of words appear in total days, one day = one count, the document frequency.\n",
    "words_freq = FreqDist(list(chain.from_iterable([set(value) for value in token_dic.values()])))\n",
    "\n",
    "# define context dependent stopwords that need to be removed from vocab\n",
    "# appear less than 5 days\n",
    "lessFreqwords = set([k for k, v in words_freq.items() if v < 5])\n",
    "# appear more than 60 days\n",
    "highFreqwords = set([k for k, v in words_freq.items() if v > 60])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can remove the context dependent stopwords from the token dictionary. Two functions are created here for each frequency threshold. And stopwords with frequency < 5 is removed first and frequency > 60 is removed next.\n",
    "\n",
    "Store the new tokens into a new token dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for removing words that meets the low frequency threshold.\n",
    "def removeLessFreqWords(date):\n",
    "    return (date, [w for w in token_dic[date] if w not in lessFreqwords])\n",
    "\n",
    "# remove low frequency words from token library (frequency < 5)\n",
    "token_dic_2 = dict(removeLessFreqWords(date) for date in token_dic.keys())\n",
    "\n",
    "# define functions for removing words that meets the high frequency threshold.\n",
    "def removeHighFreqWords(date):\n",
    "    return (date, [w for w in token_dic_2[date] if w not in highFreqwords])\n",
    "\n",
    "# remove high frequency words from token library (frequency > 60)\n",
    "token_dic_3 = dict(removeHighFreqWords(date) for date in token_dic_2.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The independent stopwords given in the file \"stopwords_en\" are removed next, read the file first. stopword list is covert to set() for faster iteration here.\n",
    "\n",
    "Same as before, define a function for stopwords removal. And apply it to every value inside the token dictionary.\n",
    "These are removed before stemming as some stopwords will be stemmed and won't be removed if trying to remove it after stemming.\n",
    "\n",
    "The new tokens is stored into token_dic_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stopwords text and covert to set for faster iteration.\n",
    "stopwords = []\n",
    "with open(\"stopwords_en.txt\") as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "stopwords_set = set(stopwords)  \n",
    "\n",
    "# stopwords is removed here as they are meanningless and compress the data.\n",
    "# These indenpendent Stopwards take large porpotion of the whole dataset\n",
    "def stopword_remove(date):\n",
    "    return(date, [w for w in token_dic_3[date] if w not in stopwords_set])\n",
    "\n",
    "# new dictionary with stopwords removed\n",
    "token_dic_clean = dict(stopword_remove(date) for date in token_dic_3.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming need to be using Porter method. PorterStemmer() is used here to stem every token in the cleaned token dictionary.\n",
    "\n",
    "Tokens that have a length less than 3 need to be removed. This is not done before stemming as stemming may creates tokens with ength less than 3. We want those to be removed as well. \n",
    "\n",
    "The tokens with length less than 3 are removed with function \"removeLengthless3\". \n",
    "\n",
    "Now, store the remaining tokens, and we have the final token dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stemming method - Porter\n",
    "def stemming(date):\n",
    "    stem = PorterStemmer()\n",
    "    return(date, [stem.stem(w) for w in token_dic_clean[date]])\n",
    "\n",
    "# stem the text here to avoid miss count of words with same meaning. \n",
    "# As told in annocement stemming should be down after context-dependent stopwords removal\n",
    "token_dic_stemmed = dict(stemming(date) for date in token_dic_clean.keys())\n",
    "\n",
    "# remove words with length less than 3 from tokens\n",
    "def removeLengthless3(date):\n",
    "    return(date, [w for w in token_dic_stemmed[date] if (len(w) > 2)])\n",
    "\n",
    "# remove words with length less than 3 after stemmed, before final output to make sure no word with length < 3 in output.\n",
    "token_dic_final = dict(removeLengthless3(date) for date in token_dic_stemmed.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use FreqDist to find the frequency of each token in each date and store in a dictionary named token_dic_unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to filter top 100 unigrams in a day\n",
    "def unigram(date):\n",
    "    return(date, FreqDist(token_dic_final[date]).most_common(100))\n",
    "\n",
    "# create dictionary contain the top 100 unigrams for each day\n",
    "token_dic_unigram = dict(unigram(date) for date in token_dic_final.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token_dic_unigram contain the frequency count and unigrams we want to output. Output to txt file in format of date:(unigram1:count),(unigram2:count)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output unigrams to txt\n",
    "\n",
    "unigram_file = open(\"100uni.txt\", \"w\")\n",
    "for k,v in token_dic_unigram.items():\n",
    "    unigram_file.write(\"{}:{}\".format(k,v) + \"\\n\")\n",
    "unigram_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams are generated using ngrams() where n = 2 means bigrams.\n",
    "\n",
    "Bigrams are output to text file in same format as unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigrams\n",
    "\n",
    "# define the rule of finding top 100 bigram, use ngrams with n = 2\n",
    "def bigram(date):\n",
    "    return(date, FreqDist(ngrams(token_dic[date], n = 2)).most_common(100))\n",
    "\n",
    "# create dictionary contain top 100 bigrams for each day\n",
    "# In order to match the sample_bigram output as close as possible.\n",
    "# Bigram is created from data that did not go through any stopwords removal\n",
    "token_dic_bigram = dict(bigram(date) for date in token_dic.keys())\n",
    "\n",
    "# output bigrams to txt\n",
    "bigram_file = open(\"100bi.txt\", \"w\")\n",
    "for k,v in token_dic_bigram.items():\n",
    "    bigram_file.write(\"{}:{}\".format(k,v) + \"\\n\")\n",
    "bigram_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate vocabulary list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary list need to contain both 200 meaningful bigrams and all unigrams.\n",
    "\n",
    "We have the unigrams, now we need to find the 200 bigrams.\n",
    "The bigrams are found using nltk.collocations.BigramAssocMeasures() with PMI. Incase there are bigrams with less than 3, they are removed as well.\n",
    "\n",
    "The vocabulary list does not require frequency, thus we can use set() to get all the unique unigrams.\n",
    "\n",
    "MWETokenizer is used to convert bigrams from (x,y) format to x_y format.\n",
    "final_vocab is the vocabulary list we wish to output, use sort() to sort it in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create vocab list\n",
    "\n",
    "# a list of all original unigram tokens \n",
    "#token_list = list(chain.from_iterable([value for value in token_dic.values()]))\n",
    "\n",
    "# vocab is formed by unique tokens in all days (which extract from tokens that are cleaned)\n",
    "vocab = list(chain.from_iterable([value for value in token_dic_final.values()]))\n",
    "vocab_set = set(vocab)\n",
    "\n",
    "# To make the bigrams as meaningfor as possible, bigrams are gernerated from tokens with all stopwards removed.\n",
    "# it is not stemmed as in bigrams results would be wierd when words are stemmed\n",
    "# create list with the tokens mentioned above.\n",
    "bigram_list = list(chain.from_iterable([value for value in token_dic.values()]))\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# find bigrams in the token lists\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(bigram_list)\n",
    "# To make sure length < 3 is removed \n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "# use pmi measures to find the top 200 bigrams\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200)\n",
    "\n",
    "\n",
    "# change from set to list\n",
    "vocab_list = list(vocab_set)\n",
    "vocab_list.sort()\n",
    "# create a identical token list but with bigrams added as unigrams\n",
    "token_list = list(vocab_set)\n",
    "\n",
    "# append bigrams into vocab list\n",
    "for i in top_200_bigrams:\n",
    "    vocab_list.append(i) # list with vocabs + 200 bigrams\n",
    "\n",
    "\n",
    "# append every single word in bigrams into the new token list\n",
    "for bi in top_200_bigrams:\n",
    "    for w in bi:\n",
    "        token_list.append(w)\n",
    "\n",
    "# use mwe tokenizer to tokenize the token_list with our vocab_list to make the result bigrams include \"_\"\n",
    "mwe_tokenizer = MWETokenizer(vocab_list)\n",
    "final_vocab = mwe_tokenizer.tokenize(token_list)\n",
    "\n",
    "# sort alphabetically\n",
    "final_vocab.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can output the result to text and assign a number to each of the word by order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output vocab list to txt\n",
    "vocab_file = open(\"vocab.txt\", \"w\")\n",
    "i = 0\n",
    "for w in final_vocab:\n",
    "    vocab_file.write(\"{}:{}\".format(w,i) + \"\\n\")\n",
    "    i = i + 1\n",
    "vocab_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate countvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, need to find all bigrams in the tokens dictionary.\n",
    "This is achieved by the filter_token function. The results are stored in a new dictionary.\n",
    "\n",
    "Next, combine the new bigram dictionary with the unigram dictionary by matching keys.\n",
    "Now, a complete dictionary with bigrams and unigrams is created.\n",
    "\n",
    "From there create count_matrix and check its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create countvec \n",
    "\n",
    "# filter out top 200 bigrams and other words contain \"_\" in each document.\n",
    "def filter_tokens(date):\n",
    "    return(date, [w for w in mwe_tokenizer.tokenize(token_dic[date]) if \"_\" in w])\n",
    "\n",
    "# Store the bigram tokens into its corresponding key(date) in the dictionary\n",
    "vocab_docu = dict(filter_tokens(date) for date in token_dic.keys())\n",
    "\n",
    "# filter out the ones that are not bigrams in our vocab\n",
    "vocab_docu_2 = dict((date, [w for w in vocab_docu[date] if w in final_vocab]) for date in vocab_docu.keys())\n",
    "\n",
    "# combine two dictionary together\n",
    "for k,v in token_dic_final.items():\n",
    "    if k in vocab_docu_2:\n",
    "        vocab_docu_2[k] = vocab_docu_2[k] + v\n",
    "\n",
    "# generate vectorizer\n",
    "#vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "# create our matrix that contains number of documents and vocab counts by joining all tokens\n",
    "#count_matrix = vectorizer.fit_transform([' '.join(value) for value in vocab_docu_2.values()])\n",
    "\n",
    "#count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output to txt file, with numbers assigned to each of words in the vocabulary. Also find the frequecy of each word by using FreqDist(). So that each word is repesented with a number same as the vocab.txt.\n",
    "Write into txt file in format of date: (vocab_no : count) for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create countVectors\n",
    "out_file = open(\"countVec.txt\", 'w')\n",
    "\n",
    "# create a dictorary to hold the numbers assigned to each vocab\n",
    "vocab_dict = {}\n",
    "i = 0\n",
    "for w in final_vocab:\n",
    "    vocab_dict[w] = i\n",
    "    i = i + 1\n",
    "\n",
    "# write to txt in format [date:(vocab_no, counts)]\n",
    "for i, d in vocab_docu_2.items():\n",
    "    d_idx = [vocab_dict[w] for w in d]\n",
    "    # write \"date,\" before writing countVectors\n",
    "    out_file.write(\"{},\".format(i))\n",
    "    # write countVectors\n",
    "    for k, v in FreqDist(d_idx).items():\n",
    "        out_file.write(\"{}:{} \".format(k,v))\n",
    "    out_file.write('\\n')\n",
    "out_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
