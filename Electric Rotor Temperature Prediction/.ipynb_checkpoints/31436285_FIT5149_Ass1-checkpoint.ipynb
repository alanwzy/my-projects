{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5149 S2 2020 \n",
    "# Assessment 1: Electric Rotor Temperature Prediction\n",
    "\n",
    "\n",
    "Student information\n",
    "    - Family Name: WANG\n",
    "    - Given Name: ZHIYIN\n",
    "- Student ID: 31436285\n",
    "- Student email: zwan0252@student.monash.edu\n",
    "\n",
    "Programming Language: R 4.0.5 in Jupyter Notebook\n",
    "\n",
    "R Libraries used:\n",
    "- tidyr\n",
    "- dplyr\n",
    "- leaps\n",
    "- car: To create scatterplot matrix\n",
    "- glmnet: To apply ridge and lasso generlization on regression.\n",
    "- mgcv : To create generalized additive model\n",
    "- randomForest : To create randomForest regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#sec_1)\n",
    "3. [Exploratory Data Analysis](#sec_3)\n",
    "3. [Methodology](#sec_4)\n",
    "3. [Model Development](#sec_5)\n",
    "3. [Results and discussion](#sec_6)\n",
    "3. [Conclusion](#sec_7)\n",
    "3. [References](#sec_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a class=\"anchor\" id=\"sec_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, the aim is design a model that can accurately predict the rotor temperatures of a permanent magnet synchronous motor (PMSM) in real-time. The model need to be designed with appropriate feature selection, that can estimate the rotor temperature (pm) and meets the following requirements:\n",
    "- small model sizes \n",
    "- high level of prediction accuracy \n",
    "- lightweight implementation\n",
    "\n",
    "The data set used is in the task is \"pmsm_temperature_data_A1_2021.csv\", contains 15,147 instances, each of which have 13 columns. The first eight columns are the predictors and the nineth column is the target variable \"pm\".\n",
    "\n",
    "There are two main tasks to complete:\n",
    "\n",
    "#### Prediction task:\n",
    "\n",
    "Split dataset in to training set and test set and use training set to perform prediction on test set\n",
    "- training set: profile_id not 72 & 81\n",
    "- test set: profile_id 72 & 81\n",
    "\n",
    "In this task, in addition to the final model, you are also required to:\n",
    "\n",
    "1. compare at least one model that is different from your final model\n",
    "2. describe and justify the choice of your models\n",
    "3. assess, analyze and interpret your results\n",
    "\n",
    "#### Inference task: \n",
    "\n",
    "Identify the key factors that have strong affect on the rotor temperature. Inference can be based on variable correlation analysis, regression equations, or any other form. \n",
    "\n",
    "1. Identify a subset of attributes that have a significant impact on the prediction of pm.\n",
    "2. Report your identification with statistical evidence (e.g. correlations, p-values) and interpret the identifed attribute subset (e.g. as to why certain attributes have certain impacts on the prediction).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading libraries used for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(tidyr)\n",
    "library(dplyr)\n",
    "library(leaps)\n",
    "library(car) #scatterplot matrix\n",
    "library(glmnet) # ridge and lasso\n",
    "library(mgcv) # gam\n",
    "library(randomForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis<a class=\"anchor\" id=\"sec_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "data = read.csv(file = \"./pmsm_temperature_data_A1_2021.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the dataset\n",
    "\n",
    "First, start with inspecting the whole dataset to get an overview of the structure and statistical summary of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inspect the dataset and display the dimensions\n",
    "cat(\"The dataset has\", dim(data)[1], \"records, each with\", dim(data)[2],\n",
    "    \"attributes. The structure is:\\n\\n\")\n",
    "\n",
    "# inspect the structure of data\n",
    "str(data)\n",
    "\n",
    "cat(\"\\n Inspect the first few rows and columns in the dataset:\")\n",
    "# Inspect the first few records\n",
    "head(data)\n",
    "\n",
    "cat(\"\\n The statistical summary for each attribute:\")\n",
    "# Statistical summary \n",
    "summary(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary above shows:\n",
    "- There are 15147 observations and 13 columns/variables in the dataset\n",
    "- The first eight variables: ambient, coolant, u_d, u_q, motor_speed, torque, i_d, i_q are the predictors for our model\n",
    "- pm is the target variable \n",
    "- The other variables will not be used.\n",
    "- Our predictors and target variables have both negative and positive values, range between -4 to 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the attributes\n",
    "\n",
    "#### Attributes summary: \n",
    "\n",
    "Here are the detail explaination of the 13 attributes in the dataset.\n",
    "\n",
    "ambient: Ambient temperature as measured by a thermal sensor located closely to the stator.\n",
    "\n",
    "Coolant: Coolant temperature. The motor is water cooled. Measurement is taken at outflow.\n",
    "\n",
    "u_d: Voltage d-component\n",
    "\n",
    "u_q: Voltage q-component\n",
    "\n",
    "motor_speed: Motor speed\n",
    "\n",
    "torque: Torque induced by current.\n",
    "\n",
    "i_d: Current d-component\n",
    "\n",
    "i_q: Current q-component\n",
    "\n",
    "pm: Permanent Magnet surface temperature representing the rotor temperature. This was measured with an infrared thermography unit.\n",
    "\n",
    "stator_yoke: Stator yoke temperature measured with a thermal sensor.\n",
    "\n",
    "stator_tooth: Stator tooth temperature measured with a thermal sensor.\n",
    "\n",
    "stator_winding: Stator winding temperature measured with a thermal sensor.\n",
    "\n",
    "profile_id: Each measurement session has a unique ID. Make sure not to try to estimate from one session onto the other as they are strongly independent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate single attributes \n",
    "\n",
    "The first eight attributes are the predictors will be used to predictor our target variable \"pm\". Therefore, we are only  investigateing these nine attributes and leave the other ones out.\n",
    "\n",
    "To understand the distribution of each variable, we used hist() function to plot histograms. Histogram can give a simple overview of the distribution of a variable, it helps to determine weather the distribution of a variable is normal or skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot each attribuite on histogram\n",
    "par(mfrow = c(2,2))\n",
    "for (i in colnames(data)[1:9]){\n",
    "    hist(data[,i], main = paste0(\"Histogram of\",\" \", i))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms shows:\n",
    "- Some attributes are skewed\n",
    "- coolant, u_q, motor_speed are right skewed\n",
    "- i_d is left skewed\n",
    "- For attributes like u_d, u_q, i_d, i_q, they all have one bar dominant the distribution indicates many values are centered in a particular range.\n",
    "- Most motor speed measured is between -1 to -1.5\n",
    "- Most rotor temperatures measured in the dataset are between -1 to 1 \n",
    "\n",
    "For variables are not normal distributed, apply transformation on them to investigate if they can be normal distributed after the transformation. This may help in the modelling process and avoid issues such as heteroskedasticity\n",
    "\n",
    "The one issue here is that all our variables have negative values. Perform transformation on attributes with negative value will produce NaNs, we need to solve this problem by adding a constant to each attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further investigation on specific attributes\n",
    "\n",
    "First, add a constant to each variable to make sure NaNs will not be produced in transformation and the shape of the distribution are not affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add min value to each of variables so they can be transformed\n",
    "data$pm_n = data$pm + 5\n",
    "data$ambient_n = data$ambient + 5\n",
    "data$coolant_n = data$coolant + 5\n",
    "data$u_d_n = data$u_d + 5\n",
    "data$u_q_n = data$u_q + 5\n",
    "data$motor_speed_n = data$motor_speed + 5\n",
    "data$torque_n = data$torque + 5\n",
    "data$i_d_n = data$i_d + 5\n",
    "data$i_q_n = data$i_q + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the new variables, apply log transformation and plot them on histograms to see if the variables have a log normal distribution.\n",
    "\n",
    "Use log trasnformation can reduce positive skewness and we will apply other transformations such as square root transformation to reduce negative skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# apply log transformation to deal with skewed data\n",
    "par(mfrow=c(2,2))\n",
    "hist(log(data$coolant_n))\n",
    "hist(log(data$u_q_n))\n",
    "hist(log(data$motor_speed_n))\n",
    "hist(log(data$i_d_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histrograms shows:\n",
    "- After log transformation, the distribution of coolant temperture and u_q improved and less skewed, but still not normal distirbuted.\n",
    "\n",
    "Apply other transformations to see if any further improvements can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# apply square root transformation\n",
    "par(mfrow = c(2,2))\n",
    "hist(sqrt(data$coolant_n))\n",
    "hist(sqrt(data$u_q_n))\n",
    "hist(sqrt(data$motor_speed_n))\n",
    "hist(sqrt(data$i_d_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# apply cube root transformation\n",
    "par(mfrow = c(2,2))\n",
    "hist((data$coolant_n)^(1/3))\n",
    "hist((data$u_q_n)^(1/3))\n",
    "hist((data$motor_speed_n)^(1/3))\n",
    "hist((data$i_d_n)^(1/3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that different transformations have different effect of countering skewness on each variables:\n",
    "- log transformation is the best for coolant\n",
    "- cube root transformation is the best for u_q\n",
    "- motor_speed and i_d still skewed after transformations\n",
    "\n",
    "Log transformation on coolant temperture and cube root tranformation on u_q can effectively reduced the skewness of these two variables and may help on future modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate attributes in pairs \n",
    "\n",
    "It is important to investigate the relationships between each variables in statistically modelling. It gives a deeper understanding of the dataset and provides guidance to the variables and interactions to include in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations between variables\n",
    "Use scatterplotMatrix() function to generate a plot includes all pairs of variables as scatterplots can clearly show if there is any potiential correlation between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each pairs of variables\n",
    "scatterplotMatrix(~ambient+coolant+u_d+u_q+i_d+i_q+motor_speed+torque+pm, data = data, main = \"Scatterplot matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot shows:\n",
    "- Strong positive correlation between i_q and torque\n",
    "- The is a strong linear relationship between i_q and torque \n",
    "- Negative correlation between u_d and torque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most pairs in the scatterplots shows a random relationship between each other, we should check the correlation coefficients of each pair to see if any strongly correlated pairs were missed as there are too many data points in each pair of graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Correlation Coefficients of each pair\n",
    "cor(data[1:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation table shows:\n",
    "- Nearly perfect positive correlation between i_q and torque\n",
    "- Strong negative correlation between u_d and torque\n",
    "- Strong negative correlation between u_d and i_q\n",
    "- Strong positive correlation between u_q and motor_speed\n",
    "- Strong negative correlation between motor_speed and i_d\n",
    "- Ambient has the highest correlation with our target variable pm\n",
    "\n",
    "These pairs are the highly correlated variables need to be paying attention in the process of modelling. These pairs can be used as interaction terms that may improve our performance mesurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate on pairs with high correlations\n",
    "\n",
    "The previous scatterplot shows I_q and torgue has a strong linear relationship. As u_d has a strong correlation to both variable, lets investigate on u_d to check if u_d have a linear relationship with the two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot u_d vs i_q and torque\n",
    "plot(data$u_d, data$i_q)\n",
    "plot(data$u_d, data$torque)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot shows:\n",
    "- when the voltage d-component increase current q-component decreases and torque induced by current decrease as well and that matches with the high correlation between i_q and torque.\n",
    "- On the other hand, in real life voltage and current have positive relationship, in this case, we understands that the d-component of voltage and q-component of current has a negative relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target variable and predictors\n",
    "Let's investigate how each predictors distribute vs the target variable rotor temputure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "par(mfrow = c(2,2))\n",
    "for (i in colnames(data[1:8])){\n",
    "    plot(data$pm, data[,i], main = paste0(\"pm vs\", \" \", i))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plots shows: \n",
    "\n",
    "- Ambient shows some linear relationship with pm, apart from it, it is hard to identify any clear linear relationship between the other predictors and the target variable. \n",
    "- The result indicates it might be hard to just build an accurate model around the orginal 8 predictors as the points are distributed randomly on the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "#### Analysis of predictors and target\n",
    "\n",
    "The target variable pm have a normal distribution shape, where some predictors show a strong skewness in their distribution.\n",
    "coolant, u_q, motor_speed are right skewed and i_d is left skewed. Applying log transformation to motor_speed shows a more normal distribution and apply cube root transformation to u_q reduced the skewness, all transformation results poorly on the other two predictors. \n",
    "\n",
    "Most variables have a large number of observations grouped in one place, indicates most observations are centered together closely. This may leave some of the other observations become outliers or strong influencers in our modelling process as the distribution of the variables are not normal but with spikes.\n",
    "\n",
    "#### Analysis of correlated pairs of variables\n",
    "\n",
    "Through investigating the relationship between rotor temperature and the eight predictors, it appear that most predictors have a low correlation with the rotor temperature. It will be hard to build a accurate linear model based on the eight variables given, there might be potiential non-linear relationships between some of the predictors and our target variable, non-linear models may be required for a better performance measurements.\n",
    "\n",
    "These are highly negative correlated: \n",
    "- Voltage d-component and torque induced by current\n",
    "- Voltage d-component and q-component induced by current\n",
    "- motor_speed and d-component induced by current\n",
    "\n",
    "Where voltage components seems to have a negative relationship with variable related to current components, which is surprisingly opposite to the relationship between voltage and current in real world. Motor speed is positively correlated with q-component of voltage and negatively correlated with d-component of current, may indicates motor speed increase when voltage\n",
    "increases and matches with our previous finding of negative relationship between voltage component and current component.\n",
    "\n",
    "Among the predictors, ambient temperature appear to be most correlated to rotor temperature, most of the observations of the two temperature measured are around zero. The coolant temperature however, does not show any clear relationship with the rotor temperature. Thus, in expectation ambient temperature will have more weight in the prediction of rotor temperature through models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology<a class=\"anchor\" id=\"sec_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The following models will be developed:\n",
    "\n",
    "- Linear regression model\n",
    "- Ridge & Lasso regression\n",
    "- Non-linear model\n",
    "- Tree model\n",
    "\n",
    "2. Feature selection\n",
    "- apply tranformations on predictors\n",
    "- add interactions of predictors\n",
    "- use step wise feature selection to reduce complexity\n",
    "\n",
    "3. Model evaluation\n",
    "- access the performance of model based on residual plots or performance measurements\n",
    "- go back to feature selection and use a different method to improve the model if possible\n",
    "\n",
    "4. Model comparision\n",
    "- perform prediction on test data for the best model of each model family\n",
    "- compute RMSE and compare the models\n",
    "\n",
    "5. Final model\n",
    "- select the model that perform the best and satisfiy all the requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development <a class=\"anchor\" id=\"sec_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataframe\n",
    "Split the data into training set and testing set according to the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# subset data into train and test set\n",
    "train = data %>% filter(profile_id != 72 & profile_id != 81)\n",
    "test = data %>% filter(profile_id == 72 | profile_id == 81)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Subset Selection (feature selection)\n",
    "\n",
    "Before any model development, use the regsubsets() function (part of the leaps library) to performs best subset selection and identifying the best model that contains a given number of predictors to check if all predictors should be included in our model development. The feature selection is performed here because the correlation between our predictors and target variable was shown to be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform best subset selection\n",
    "reg_fit = regsubsets(pm~ ambient + coolant + u_d + u_q + motor_speed + torque + i_d + i_q, data = train)\n",
    "reg_summary = summary(reg_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use code from Lab 6 to plot RSS,adj R squared, Cp and Bic graph to find the optimal number of variables to include in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "par(mfrow = c(1,2))\n",
    "\n",
    "# Find the maximum adj_r2\n",
    "adj_r2_max = which.max(reg_summary$adjr2)\n",
    "# Find the minimum Cp and BIC\n",
    "cp_min = which.min(reg_summary$cp)\n",
    "bic_min = which.min(reg_summary$bic) \n",
    "\n",
    "# plot the measurements for each number of variables and \n",
    "# show the maximum and minmum on the plots as red points\n",
    "# RSS\n",
    "plot(reg_summary$rss, xlab = \"Number of Variables\", ylab = \"RSS\", type = \"l\")\n",
    "\n",
    "# adjusted R2\n",
    "plot(reg_summary$adjr2, xlab = \"Number of Variables\", ylab = \"Adjusted RSq\", type = \"l\")\n",
    "points(adj_r2_max, reg_summary$adjr2[adj_r2_max], col =\"red\", cex = 2, pch = 20)\n",
    "\n",
    "# Cp\n",
    "plot(reg_summary$cp, xlab = \"Number of Variables\", ylab = \"Cp\", type = \"l\")\n",
    "points(cp_min, reg_summary$cp[cp_min], col = \"red\", cex = 2, pch = 20)\n",
    "\n",
    "# BIC\n",
    "plot(reg_summary$bic, xlab = \"Number of Variables\", ylab = \"BIC\", type = \"l\")\n",
    "points(bic_min, reg_summary$bic[bic_min], col = \"red\", cex = 2, pch = 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots shows:\n",
    "- RSS decreases as numbers of variables included increases\n",
    "- Adjusted R-squared is maximum at 8 variables\n",
    "- Cp is minimum at 8 variables\n",
    "- BIC is minimum at 6 variables and does not increase significantly at 8 variables\n",
    "\n",
    "The plots all agree that a model with 4 or fewer predictors is insufficient as all measurements show that the model has a significant improvment above 4 predictors. There is no clear sign of overfitting when all eight predictors are included.\n",
    "BIC is minimum at 6 variables it indicates that we should include 6 or all variables in our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model\n",
    "First, start with building a simple linear regression model with all eight predictors included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "fit1 = lm(pm~ambient + coolant + u_d + u_q + motor_speed + torque + i_d + i_q, data = train)\n",
    "# check for statistics and residual plots\n",
    "summary(fit1)\n",
    "plot(fit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted R-squared (ùëÖ2) value indicates this model explains 49.1% of the variation in rotor temperature.\n",
    "\n",
    "The F-statistic 1167 has a p-value < 2.2e-16 - which reject the null hypothesis (the model is not useful) - the model is significantly useful at significance level of 0.001.\n",
    "\n",
    "The p-values for the coefficients show all the variables are significant at the 0.05 level. The p value of I_d and i_q is relatively higher than the other variables, indicates the current components are not be as important as other predictors in this model.\n",
    "\n",
    "Check the residuals using the plot function, the plot shows:\n",
    "\n",
    "- Residual vs Fitted - shows the residuals are reasonably distributed around zero, but not as evenly distributed as desired where more residuals centered at middle and have a higher residual value. The residuals are relatively less on the sides and more close to the mean. The plot indicates that the model may violates the assumption of homoscedasticity because the error terms change along the regression line.\n",
    "\n",
    "- Normal Q-Q - Most residuals sits accurately on the dashed line, the residuals only deviate away from the dashed line on higher quantiles > 2, indicating the residuals may are not be perfect normally distributed but is still acceptable.\n",
    "\n",
    "- Scale-Location - The chart shows no specific pattern, the points are distributed randomly indicates the model did not validate the assumption of equal variance.\n",
    "\n",
    "- Residuals vs Leverage - The chart shows there may be some outliers but no influencial points. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some interaction term to the linear model see if it improves the fit. The interaction terms are selected according to pairs of variables with high correlation as they might be significant in the prediction of temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# adding interaction terms that are highly coorelated\n",
    "fit2 = lm(pm~ ambient + coolant + u_d + u_q + motor_speed + torque + i_d + i_q + i_q:torque + u_d:torque + u_d:i_q\n",
    "               + u_q:motor_speed + motor_speed:i_d, data = train) \n",
    "# check for statistics and residual plots\n",
    "summary(fit2)\n",
    "plot(fit2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intersections has improved the adjusted R-squared value to 55% indicates the model explains 55% of the variance in rotor temperature.\n",
    "\n",
    "F-statistic of 911.6 has a p-value < 2.2e-16 - which reject the null hypothesis (the model is not useful) - the model is significantly useful at significance level of 0.001.\n",
    "\n",
    "The intersections and variables are all significant at 0.05 level, and include intersections in the model does improve adjusted R-squared value indicates the intersections are important predictors should be included in the linear model.\n",
    "\n",
    "Check the residuals using the plot function, the plot shows:\n",
    "- The residuals vs fitted plot has not improved but is affected by some influencial points, the mean line is pointing down due to the three influencial points. On the other hand, the model still suffers from heteroskedasticity where the spread of residuals are not constant.\n",
    "- The other plots also shows influencial points, on the residual vs leverage plot point 4192, 4162 and 4172 are clearly influencial points where they are beyond the cook's distance line. \n",
    "\n",
    "The intersections added in this model appears to have strong impact on the model performance. These intersections are variables with high correlations, they successfuly improve the adjuested R-squared of the model and the p values of the intersections indicates all of them are siginificantly important to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the influential points in fit2 : 4162,4172,4192 to see if there is any unusual value in the predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check the influencial points\n",
    "train[c(4192,4162,4172),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data exploration, i_q has investigated to have a 0.996 correlation with torque.\n",
    "\n",
    "In the table shown above, the perfect correlation between i_q and torque does not match with the values in these rows.\n",
    "If we compare these three rows, most values of predictors are nearly equal in each row. However, the torque and motor_speed in these rows are different from each other. The difference in torque in this case is a unusual event, thus causing these three rows become influential points and affect the accuracy of the model.\n",
    "\n",
    "Data used for training has total 9652 observations, in this case we choose to exclude these 3 rows first and see how our model performs. \n",
    "\n",
    "After fitting the model with out the three rows, new influential point appears. Removing data from our training data may not be approiate as they are also real data and the unusual values are caused by other variables we have not identified yet.\n",
    "\n",
    "But, let's try remove the new influencial points as well and see how they impact our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# exclude the three rows and include other interactions see if it improves the model \n",
    "fit3 = lm(pm~ ambient + coolant + u_d + u_q + motor_speed + torque + i_d + i_q + i_q:torque + u_d:torque + u_d:i_q\n",
    "               + u_q:motor_speed + motor_speed:i_d, data = train[-c(116,4162,4172,4192,4221,4201),])\n",
    "# check for statistics and residual plots\n",
    "summary(fit3)\n",
    "plot(fit3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After remove the influencial points, the model seems perform better, the adjusted R-squared value is 65% indicates the model explains 65% of the variance in rotor temperature.\n",
    "\n",
    "F-statistic of 1404 has a p-value < 2.2e-16 - which reject the null hypothesis (the model is not useful) - the model is significantly useful at significance level of 0.001.\n",
    "\n",
    "The p values improves significantly compare to previous model, where all variables are all significant at 0.001 level.\n",
    "\n",
    "Check the residuals using the plot function, the plot shows:\n",
    "- Residual vs fitted plot: The spread of residuals still decrease as fitted value increases, but there is no more influencial points and the line fitted is back to linear.\n",
    "- No more influenial point in residuals vs leverage plot\n",
    "- However the model still suffers from heteroskedasticity \n",
    "\n",
    "As mentioned before,remove influencial points is not the best idea, in the next model influencial points will not be removed, as the influencial points may be connected to the heteroskedasticity issue that all previous model has suffered from. Add log transformation and square root transformations might be helpful, lets try to fit the model with transformation of skewed variables identified in the EDA part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's add weights to our formula and see if it can solve heteroskedasticity as the weighted least squares corrects the non-constant variance and the residuals in previous models all follows an decreasing pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate weighted least squares\n",
    "weights = 1 / lm(abs(fit1$residuals) ~ fit1$fitted.values)$fitted.values^2\n",
    "# add log and cube root transformation\n",
    "fit4 = lm(pm~ ambient + coolant + u_d + u_q + motor_speed + torque + i_d + i_q + i_q:torque + u_d:torque + u_d:i_q\n",
    "               + u_q:motor_speed + motor_speed:i_d + log(motor_speed_n) + I((u_q)^(1/3)),\n",
    "          data = train,\n",
    "          weights = weights)\n",
    "# check for statistics and residual plots\n",
    "summary(fit4)\n",
    "plot(fit4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted R-squared (ùëÖ2) value indicates this model explains 79.5% of the variation in rotor temperature.\n",
    "\n",
    "The F-statistic 1183 has a p-value < 2.2e-16 - which reject the null hypothesis (the model is not useful) - the model is significantly useful at significance level of 0.001.\n",
    "\n",
    "The p-values for the coefficients show most the variables are significant at the 0.001 level. However there are two predictors have p value is larger than 0.05, motor_speed and torque. This indicates that in this model these two variables should be removed.\n",
    "\n",
    "Check the residuals using the plot function, the plot shows:\n",
    "\n",
    "- Residual vs Fitted - the plot has a significant improvement compare to previous models, the residuals are mroe evenly distributed around zero, but the fitted line is not as linear as desired. The pattern of decrease in residuals as fitted values increase has improved as well. The model may still suffer from heteroskedasticity but relatively at a smaller degree.\n",
    "\n",
    "- Normal Q-Q - Most residuals sits accurately on the dashed line, the residuals only deviate away from the dashed line on higher quantiles > 2, indicating the residuals may are not be perfect normally distributed but is still acceptable.\n",
    "\n",
    "- Scale-Location - The chart shows no specific pattern, the points are distributed randomly indicates the model did not validate the assumption of equal variance.\n",
    "\n",
    "- Residuals vs Leverage - The chart shows there are no influencial points outside the cook's distance line, there are outliers but not considered as influencial points to our model. \n",
    "\n",
    "The cube root transformation and log transformation discovered in EDA shows a strong impact on the model, improves the adjusted R-square from 65% to 80%. The p values of these two transformed variable indicates they are significant and a important contribution to the prediction of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed torque and motor speed from fit 4\n",
    "fit5 = lm(pm~ ambient + coolant + u_d + u_q + i_d + i_q + i_q:torque + u_d:torque + u_d:i_q\n",
    "               + u_q:motor_speed + motor_speed:i_d + log(motor_speed_n) + I((u_q)^(1/3)),\n",
    "          data = train,\n",
    "          weights = weights)\n",
    "# check for statistics and residual plots\n",
    "summary(fit5)\n",
    "plot(fit5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove torque and motor speed from the model because their p values are larger than 0.05, the results are similar to fit4, but the model size is smaller and the model complexity is reduced. \n",
    "The residuals on the residual vs leverage plot appear to be slightly more evenly distributed, indicates heteroskedasticity is further reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stepwise Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about include all interactions in the model and use step() function to let the computer select the predictors automatically for us, see if the results will be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# include all interactions in the model\n",
    "fit6 = lm(pm ~ (ambient + coolant + u_d + u_q + motor_speed + torque + i_d + i_q)^2, data = train)\n",
    "# apply stepwise selection on fit5\n",
    "fit6_step = step(fit6)\n",
    "# print summary\n",
    "summary(fit6_step)\n",
    "plot(fit6_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shows:\n",
    "- seven predictors are removed in the step process \n",
    "- Adjusted R-squared shows 63% of the variances in rotor temperature can be explained\n",
    "- the residual plots shows influencial points same as fit 2\n",
    "\n",
    "The step wise selection shows better result compare to fit2 but can not avoid influencial points and heteroskedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression: Ridge Regression and the Lasso regression (regularization)\n",
    "\n",
    "Next, let's try apply regularization on linear model to minimize the model size and shrink the coeffcients, as in the requirements the model needs to be in small size and lightweight implementation. In this case, apply both L1 and L2 and compare them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform Ridge regression, we need to split predictors and target\n",
    "train_x = model.matrix(pm~ (ambient + coolant + u_d + u_q + motor_speed + torque + i_d + i_q)^2, train)[,-1] \n",
    "train_y = train$pm\n",
    "test_x = model.matrix(pm~ (ambient + coolant + u_d + u_q + motor_speed + torque + i_d + i_q)^2, test)[,-1]\n",
    "test_y = test$pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use cross validation to find lamda that minimize MSE \n",
    "cv_ridge = cv.glmnet(train_x, train_y, alpha = 0)\n",
    "# plot to visualize MSE vs lamda\n",
    "plot(cv_ridge)\n",
    "# show the lamda that minimizes MSE\n",
    "best_lamda = cv_ridge$lambda.min \n",
    "best_lamda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through use of cross validation and from the plot, the best lambda apply in the Ridge regression that gives the lowest MSE is found to be 0.05. This indicates that there may be no need for regularization as the lambda is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a ridge regression model to train set\n",
    "grid = 10^seq(10, -2, length = 100)\n",
    "ridge_model = glmnet(train_x, train_y, alpha = 0, lambda = grid , thresh = 1e-12)\n",
    "# predict on test set with best lamda\n",
    "ridge_pred = predict(ridge_model, s = best_lamda, newx = test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do the same for lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use cross validation to find lamda that minimize MSE \n",
    "cv_lasso = cv.glmnet(train_x, train_y, alpha = 1)\n",
    "# plot to visualize MSE vs lamda\n",
    "plot(cv_lasso)\n",
    "# show the lamda that minimizes MSE\n",
    "best_lamda_la = cv_lasso$lambda.min \n",
    "best_lamda_la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as ridge regression, the lambda shows to be really small. Indicates a very small penalty is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a lasso regression model to train set\n",
    "lasso_model = glmnet(train_x, train_y, alpha =1, lambda = grid , thresh = 1e-12)\n",
    "# predict on test set with best lamda\n",
    "lasso_pred = predict(lasso_model, s = best_lamda_la, newx = test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show the coefficients that is not zero in this lasso regression (show the predictors used)\n",
    "lasso_coef = predict(lasso_model, type = \"coefficients\", s = best_lamda)[1:14,]\n",
    "lasso_coef[lasso_coef != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to ridge regreesion that only shrink the co-effecients, lasso regression has some thing special. The lasso regression can perform feature selection, where in this case, the lasso regression gives the lowest MSE only requires 7 predictors which matches with the requirement of small model size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying linear regression model, let's try to model the data with a non-linear approch.\n",
    "\n",
    "Generalized additive model is used in this case as it allows for non-linear relationships between each feature and the target variable. The model will use smooth non-linear function for each variable, but ambient appears to have a moderate linear relationship with pm, a correlation of 0.5, therefore ambient will be kept as linear in the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized additive model\n",
    "fit7_gam = gam(pm~ ambient + s(coolant) + s(u_d) + s(u_q) + s(motor_speed) + s(torque) + s(i_d) + s(i_q) + i_q:torque + \n",
    "              u_d:torque + u_d:i_q\n",
    "               + u_q:motor_speed + motor_speed:i_d, data = train)\n",
    "summary(fit7_gam)\n",
    "plot.gam(fit7_gam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P values of most of the variables in the summary shows that they are significant at level 0.001.\n",
    "\n",
    "The adjusted R-square shows that 64.6% variance in the rotor temperature can be explained by this model.\n",
    "\n",
    "Looking at the plots:\n",
    "Iq and torque plots shows a strong non-linear fit which gives strong evidence that a non-linear function (smoothed function) is required for this variable. \n",
    "\n",
    "The GAM have much higher R-square compare to the linear model may indicates this is a better fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Trees (Random forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying linear model and non-linear model, tree-based model is another method of modelling that can be applied to our data. Decision trees can be applied as regression trees and classification trees, in this case regression trees are suitable as our target variable pm is quantitative.\n",
    "\n",
    "Use simple regression trees may not perform well compare to our models above. However, use methods such as random forest usually can improve the predictive performance of regression trees. Compare to bagged trees, each split in random forest consider is only a subset of the predictors. Becuase random forest use a smaller m value in the model, give more of a chance to each predictors in the modelling process to reduce variance and test error.\n",
    "\n",
    "In this model, (mtry = 1) is used. Means only one predictor to be considered in each split of tree, this is to make our prediction to be as accurate as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit a randomforest tree \n",
    "fit_rf = randomForest(pm~ambient + coolant + u_d + u_q + motor_speed + torque + i_d + i_q, data = train, \n",
    "                      mtry = 1, importance = TRUE)\n",
    "# show important variables\n",
    "importance(fit_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use importance() function shows:\n",
    "\n",
    "Across all of the trees considered in this random forest, ambient temperature and the coolant temperature are the two most important variables in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results and discussion <a class=\"anchor\" id=\"sec_6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation and comparison\n",
    "\n",
    "- RSS and R squared are not suitable for selecting the best model among a collection of models with different numbers of predictors\n",
    "\n",
    "- Use other measurements that add penalty to RSS for the number of variables (i.e.complexity) in the model.\n",
    "\n",
    "- Use RMSE, AIC and adjusted R squared as performance measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use the RMSE function from the house_price example \n",
    "RMSE <- function(predicted, target) {\n",
    "    se <- 0\n",
    "    for (i in 1:length(predicted)) {\n",
    "        se <- se + (predicted[i]-target[i])^2\n",
    "    }\n",
    "    return (sqrt(se/length(predicted)))\n",
    "}\n",
    "\n",
    "# function to produce measurements for model comparision\n",
    "model.accuracy = function(model){\n",
    "    pred = predict(get(model), test)\n",
    "    RMSE = RMSE(pred, test$pm)\n",
    "    AIC = AIC(get(model))\n",
    "    R2 = summary(get(model))$r.sq\n",
    "    Result = data.frame(RMSE,AIC,R2)\n",
    "    return (Result)    \n",
    "}\n",
    "\n",
    "# dataframe to store measurements\n",
    "model_comp = data.frame(RMSE = numeric(), \n",
    "                         AIC = numeric(),\n",
    "                         R2 = numeric())\n",
    "# names of models to compare\n",
    "models = c(\"fit1\", \"fit2\", \"fit3\",\"fit4\",\"fit5\", \"fit6_step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compare the linear regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute RMSE for each linear model\n",
    "for (i in models){\n",
    "    model_comp = rbind(model_comp, model.accuracy(i))\n",
    "    rownames(model_comp) = 1:nrow(model_comp)\n",
    "}\n",
    "model_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE is the error function used here to measure the performance of a model, it is the average squared difference between the estimated values and the actual value. AIC is used for evaluation of the fit and complexity of the model. If a model has a lower AIC then another indicates it is a less complex model with a good fit. In this case , the evaluation of the model will be base on RMSE, AIC and adjusted R-squared. A model with a low RMSE, low AIC and a high adjusted R-sqaured is desired.\n",
    "\n",
    "From the table, the RMSE of fit 4 and fit 5 is NaN, this is caused by an unknown issue. Fit 4 and fit 5 is the model that includes cube root transformations and weights which reduce the heteroskedasticity in the model significantly and increase adjusted R-squared by a large amount. However, for some unknown reason, the cube root transformations in the model leads to NaN in predictions. Compare the three performance measurements of fit4 and 5 to the rest of the linear models, these two models are the best. However, as the RMSE of these two models shows NaN due to unknown issue, it is a shame that these models can not be used as the final model.\n",
    "\n",
    "Fit3 has a low RMSE and high R2, however it is due to the fact that influencial points are excluded in the model. A large improvement in the three measurements shows that the removal of the influencial points have a large impact on the model. These influencial points are also real data, as the model fail to capture them fit 3 does not count as a good fit.\n",
    "\n",
    "Overall, the best linear regression model that is applicable in this case is fit6, the model with stepwise function, the model has a lower AIC compare to fit 2 and fit 1, where the RMSE and R squared is also accpetable.\n",
    "\n",
    "Only if the RMSE for fit 4 and fit 5 can be calculated, fit 5 should be the best linear model in that case. Fit 5 excluded unnecessary variables in fit4 and successfully reduced heteroskedasticity in the model. The AIC of fit 5 is extreme low compare to other models indicates it is a good fit to our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare fit6 with fit2 using anova\n",
    "anova(fit2,fit6_step,test=\"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, use anova test on fit2 and fit6, the result shows strong evidence that fit 6 is a better fit than fit 2, p value = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute RMSE for ridge and lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat(\"RMSE of ridge regression:\",RMSE(ridge_pred,test_y),\"\\n\")\n",
    "cat(\"RMSE of lasso regression:\",RMSE(lasso_pred,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE of ridge and lasso regression is relatively higher than other models indicates an lower accuracy in prediction.\n",
    "However, lasso regression is the model with smallest size where only 7 predictors are included, which satisfied the requirement of small model size and lightweight implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute performance measurement for GAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat(\"RMSE of GAM:\",RMSE(predict(fit7_gam,test),test$pm),\"\\n\")\n",
    "cat(\"AIC of GAM:\",AIC(fit7_gam),\"\\n\")\n",
    "cat(\"R2 of GAM:\",summary(fit7_gam)$r.sq,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to our best applicable linear regression model fit 6, GAM has a lower RMSE of 0.605, a low AIC of 17726 and a slightly higher R2 of 0.644. In this case, GAM with the non-linear function on the variables out performs the linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute RMSE for random forest, AIC is not fitted for random forest as it is not fitted using maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = predict(fit_rf, test)\n",
    "RMSE(rf_pred, test$pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest give a poor prediction on our test dataset and trees are harder to implement where computation times and computation power required is much larger comapre to other models. In this case, as the prediction accuracy is low and model is not easy to implement, it will not be considered as the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing all models, GAM appear to be the best model and should be our final model. As GAMs allow a non-linear fit to the variables, due to the fact that the predictors does not show a strong linear relationship with the rotor temperature, this non-linear approach performs better than linear regression model. Using a GAM model also saves times for manually trying out different transformations on each variable individually. The performance measurements supports GAM as the best model so far, a high RMSE indicates a high prediction accuracy, a low AIC compare to other models meaning the GAM model is relatively less complex, thus the model size is smaller with eight variables included and 5 pairs of intersections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 6. Conclusion<a class=\"anchor\" id=\"sec_7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, in the development of modelling rotor temperature, the result shows all eight original variables should be included in our final model. Although the correlations of the predictors and rotor temperature are low and does not appear to have a linear relationship on scatter plots. These predictors can still be modelled by smoothed non-linear function and used to predict rotor temperature to some extent. In our best model GAM, the p values of all eight variables are significant at level 0.001, means all eight variables are significant to the prediction of rotor temperature. Apart from that, five intersections are also included in the model, as they are pairs of variables with high correlation, which also have p values less than 0.001. Cube root transformation of q component of voltage has found to be strongly helpful in the modelling process, however causes unknown errors in prediction process thus can not be used in models. \n",
    "\n",
    "Among all models developed, generalized additive model performs the best based on RMSE, AIC and adjusted R squares.The non-linear fits applied on the variables have proven to produce more accurate predictions while keeping the model size as small as possible. Also, compare to other models such as randomforest trees, GAM is more easy to implement in real world. Thus, the GAM model is most suitable in this case as it  statisfied three requirements of:small model sizes, high prediction accuracy and lightweight implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
