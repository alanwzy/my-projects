{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1 Task1\n",
    "#### Student Name: ZHIYIN WANG\n",
    "#### Student ID: 31436285\n",
    "\n",
    "Date: 13/09/2020\n",
    "\n",
    "Version: 3.0\n",
    "\n",
    "Environment: Python 3.6.0 \n",
    "\n",
    "Libraries used:\n",
    "* os (for readingfiles given path, included in Anaconda Python 3.6)\n",
    "* re 2.2.1 (for regular expression, included in Anaconda Python 3.6) \n",
    "* langid.classify (for classifying language of text, included in Anaconda Python 3.6)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This assessment touches the very first step of analyzing textual data, i.e., extracting data from semi-structured text files. Each student is provided with a data-set that contains information about COVID-19 related tweets.\n",
    "The task is to extract the data and transform the data into the XML format with the following\n",
    "elements:\n",
    "1. id: is a 19-digit number.\n",
    "2. text: is the actual tweet.\n",
    "3. Created_at: is the date and time that the tweet was created\n",
    "\n",
    "And the following constraints must also be satisfied:\n",
    "1. The “id”s must be unique, so if there are multiple instances of the same tweets, you must\n",
    "only keep one of them in your final XML file.\n",
    "\n",
    "2. The non-english tweets should be filtered out from the dataset and the final XML should\n",
    "only contain the tweets in English language. For the sake of consistency, you must use\n",
    "the langid package to classify the language of a tweet.\n",
    "\n",
    "3. The re, os, and the langid packages in Python are the only packages that you are\n",
    "allowed to use for the task 1 of this assessment (e.g., “pandas” is not allowed!). Any\n",
    "other packages that you need to “import” before usage is not allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from langid import classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examining and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, file path will be defined, all files will be inside the folder <student_id> and the folder and this code file will be under the same path. \n",
    "\n",
    "Before reading all files, empty lists and dictionary is defined to hold the information in the files. All files will be loaded into a dictionary with key = file name, value = text in the file. Thus, each file is arranged and easy to access.\n",
    "\n",
    "Files will be loaded with encoding = utf-8, this will cause the emojis in the twitters can't be displayed normally. As they need to be loaded with utf-16. However, python only supports encoding with utf-8, this issue need to be fixed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define files pathways\n",
    "path = './31436285'\n",
    "all_files = os.listdir(path)\n",
    "\n",
    "# define list to contain twitter information\n",
    "text_list=[]\n",
    "id_list = []\n",
    "date_list = []\n",
    "\n",
    "# read txt fiels into a dictionary:  keys: title of text file (date) , value: data\n",
    "data_dict = {}\n",
    "\n",
    "# read all text files and store in list\n",
    "for file in all_files:\n",
    "    with open(os.path.join(path, file),'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    f.close()\n",
    "    data_dict[file] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract and transform data\n",
    "\n",
    "In this step, data is extracted from the dictionary using re.findall and regex. Three list stores different data corresponding to text, id and date.\n",
    "\n",
    "The regular expression used for text finds all text that is after \"text:\" and before the next quotation mark. The re.findall expression returns the text inside the (.*?) which is anything occurs 0+ and the \"?\" is used here to match using as few characters as possible.\n",
    "\n",
    "When finding id with regular expression, (\\d+) is used to match 1+ digitals after \"id:\"\" and same for matching date. (?:) is used while matching date so that the result matching regex inside the bracket is not retured as we only want the entire date to be returned.\n",
    "\n",
    "In the requirement, the “id”s must be unique. Thus id is checked first before any further data wrangling. A cage list is created, when an tweet is processed, its id will be added to the list. Next time, tweet with same id appear, it will be ignored.   \n",
    "\n",
    "The goal is to write english twitters to xml in correct format. Thus, non-english should not be recorded. Langid is used here to classify twitters in english. Text that are classified as english will be arranged into correct format. Then the text will be appended to new_list that used to stores all text that is being processed and ready to write into xml.\n",
    "\n",
    "However, before classifying , the issue with emoji need to be fixed here, otherwise it will effect the results of classify().\n",
    "Emojis are in format of \\uxxxx, but with two backslash \\\\ instead of single \\. eval() and replace() is used to repair the issue. Backslashes are replaced, and eval() so that text with emojis can be encoded with utf-16 to correctly display emojis.\n",
    "\n",
    "After extraction and classification of twitter data, the twitter data need to be written in format of (id + text) for twitters in every day. The format is pre-set in this make_tweet function, so it can be applied later to every file in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for extracting tweets into corresponding lists and write into xml in correct format\n",
    "def make_tweet(dict_key): \n",
    "    \n",
    "    # define list for saving output \n",
    "    new_list = []\n",
    "    # define list for id checking\n",
    "    cage = []\n",
    "    global date_list\n",
    "    \n",
    "    # extract text, id ,date from the twitter list\n",
    "    text_list = re.findall(re.compile('\"text\":\"(.*?)\"'), dict_key)  # find text with pattern as regex defined\n",
    "    id_list = re.findall(r'\"id\":\"(\\d+)\"', dict_key) # find id\n",
    "    date_list = re.findall(r'\"created_at\":\"(\\d+(?:\\W\\d{2}){2})\\w', dict_key) # find date\n",
    "    \n",
    "    \n",
    "    #global t_date\n",
    "    #t_date = date_list # copy of date_list\n",
    "    \n",
    "    # Before loop to find tweets only in english, need to solve the encoding issue of emojis in each tweet\n",
    "    for i in range(len(text_list)):\n",
    "        if bool(re.search(r'(\\\\u.*?)', text_list[i])) == True:\n",
    "            # change the \"\\\\\" of emoji into \"\\\"  and other \"\\\" issues\n",
    "            text_list[i] = text_list[i].rstrip(\"\\\\\")\n",
    "            a = eval(repr(text_list[i]).replace('\\\\\\\\', '\\\\'))\n",
    "            a = a.replace('\\\\n\\\\n','\\\\n')\n",
    "            a = a.encode('utf-16', 'surrogatepass').decode('utf-16') # encode and decode the tweet again\n",
    "            text_list[i] = a\n",
    "\n",
    "    # loop to find tweets only in english and write in correct format.\n",
    "    for i in range(len(text_list)):\n",
    "        # id check againest the appended id list, only going next step if id is not duplicated.\n",
    "        if (str(id_list[i])) not in cage:\n",
    "            # check if tweet is in english \n",
    "            if (classify(text_list[i])[0]) == 'en':\n",
    "                    # output string in correct format \n",
    "                    new_list.append(f\"<tweet id=\\\"{id_list[i]}\\\">\"+ f\"{text_list[i]}</tweet>\")\n",
    "        cage.append(str(id_list[i]))  # add this id to list, this id will be checked next time\n",
    "        \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_list is returned here with text and id output to strings in the format(\"<tweet id=\\\"{id_list[i]}\\\">\"+ f\"{text_list[i]}</tweet>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Output data to xml \n",
    "\n",
    "This is the section where formated data get to written into xml file.\n",
    "\n",
    "The only issue in this section is to make sure the xml tags are correctly written. The xml need to start with <data>, followed by a <tweets date = > tag. For each new day, this date tag need to be created. In the files, some of the files are twitters in the same date. Thus, the date tag is conditional when writing pre-processed text to each file. The date tag is only needed when starts to write tweets information in a new day. This is achieved through checking date againest date list for each file. After writing of tweets in one file is done, the date of that file will be appended into a list and this date will be checked when writing other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to output the string in correct format in the \"make tweet\" function into xml\n",
    "def write_file():  \n",
    "    # define the list for date checking, if date = date of last file, don't close <tweets> (don't write </tweets>)\n",
    "    date_cage = []\n",
    "    # count number of files\n",
    "    count = 0\n",
    "    \n",
    "    # Output data into xml\n",
    "    f = open('31436285.xml','w',encoding='utf-8')\n",
    "    # header tag\n",
    "    f.write(\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\" + \"\\n\")\n",
    "    f.write(\"<data>\"+ \"\\n\")\n",
    "    \n",
    "    # write to xml \n",
    "    for i in data_dict:\n",
    "        file_name = str(i) # define file name\n",
    "        print(file_name)\n",
    "        # prcess a single day, single file with the make_tweet function\n",
    "        tweets = make_tweet(data_dict[file_name])\n",
    "        \n",
    "        # if it is a new date, write end tag for last date and a new xml date title\n",
    "        if date_list[0] not in date_cage:\n",
    "            # except for the first file\n",
    "            if count != 0:\n",
    "                f.write(\"</tweets>\"+\"\\n\")\n",
    "            f.write(f\"<tweets date=\\\"{date_list[0]}\\\">\"+'\\n')\n",
    "            \n",
    "        # write the output from make_tweet function into xml    \n",
    "        for line in tweets:\n",
    "            f.write(\"%s\\n\" % line)\n",
    "        \n",
    "        count =+1  # count + 1\n",
    "        date_cage.append(date_list[0]) # append date to checking list\n",
    "    f.write(\"</tweets>\"+\"\\n\")\n",
    "    # close tag at the end\n",
    "    f.write(\"</data>\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Run and write to xml\n",
    "\n",
    "Finally, run the function and output all formated twitter information to xml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
